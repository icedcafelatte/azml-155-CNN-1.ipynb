{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fashion_3",
      "provenance": [],
      "collapsed_sections": [
        "SxuiF1NYX8Lh",
        "VJkZC2P_Z3bi",
        "6VW4nNCbkxAg"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLQjQSVeX0gZ"
      },
      "source": [
        "## Keras CNN으로 패션 아이템 구분하기 ##"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SxuiF1NYX8Lh"
      },
      "source": [
        "### 1. 패키지 수입 및 파라미터 지정"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GnzYTz-NXuAp"
      },
      "source": [
        "# 패키지 수입\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "from time import time\r\n",
        "from keras.datasets import fashion_mnist\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.utils import np_utils\r\n",
        "\r\n",
        "from keras.layers import Flatten, Dense, MaxPool2D, Conv2D, InputLayer"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gOprAfA9ZmFA"
      },
      "source": [
        "# 파라미터\r\n",
        "MY_EPOCH = 300\r\n",
        "MY_BATCH = 100"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJkZC2P_Z3bi"
      },
      "source": [
        "### 2. 데이터 처리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Je_0yabWZ13I",
        "outputId": "2d8906f8-76e5-48d8-8173-c6719d16cccd"
      },
      "source": [
        "# 데이터 불러오기\r\n",
        "(X_train, Y_train), (X_test, Y_test) = fashion_mnist.load_data()\r\n",
        "\r\n",
        "print('학습용 입력 데이터: ', X_train.shape)\r\n",
        "print('학습용 출력 데이터: ', Y_train.shape)\r\n",
        "\r\n",
        "print('평가용 입력 데이터: ', X_test.shape)\r\n",
        "print('평가용 출력 데이터: ', Y_test.shape)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "학습용 입력 데이터:  (60000, 28, 28)\n",
            "학습용 출력 데이터:  (60000,)\n",
            "평가용 입력 데이터:  (10000, 28, 28)\n",
            "평가용 출력 데이터:  (10000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ltODnOciekbg",
        "outputId": "448165f2-1987-49c1-c30e-6eab04f06ac6"
      },
      "source": [
        "# 데이터 샘플 출력\r\n",
        "print('학습용 데이터 첫 번째 이미지 화소 정보')\r\n",
        "print(X_train[0])\r\n",
        "plt.figure(figsize=(10,10))\r\n",
        "plt.imshow(X_train[0], cmap='gray')\r\n",
        "print('\\n학습용 데이터 첫 번째 이미지의 라벨: ', Y_train[0])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "학습용 데이터 첫 번째 이미지 화소 정보\n",
            "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   1   0   0  13  73   0\n",
            "    0   1   4   0   0   0   0   1   1   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   3   0  36 136 127  62\n",
            "   54   0   0   0   1   3   4   0   0   3]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   6   0 102 204 176 134\n",
            "  144 123  23   0   0   0   0  12  10   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0 155 236 207 178\n",
            "  107 156 161 109  64  23  77 130  72  15]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   1   0  69 207 223 218 216\n",
            "  216 163 127 121 122 146 141  88 172  66]\n",
            " [  0   0   0   0   0   0   0   0   0   1   1   1   0 200 232 232 233 229\n",
            "  223 223 215 213 164 127 123 196 229   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0 183 225 216 223 228\n",
            "  235 227 224 222 224 221 223 245 173   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0 193 228 218 213 198\n",
            "  180 212 210 211 213 223 220 243 202   0]\n",
            " [  0   0   0   0   0   0   0   0   0   1   3   0  12 219 220 212 218 192\n",
            "  169 227 208 218 224 212 226 197 209  52]\n",
            " [  0   0   0   0   0   0   0   0   0   0   6   0  99 244 222 220 218 203\n",
            "  198 221 215 213 222 220 245 119 167  56]\n",
            " [  0   0   0   0   0   0   0   0   0   4   0   0  55 236 228 230 228 240\n",
            "  232 213 218 223 234 217 217 209  92   0]\n",
            " [  0   0   1   4   6   7   2   0   0   0   0   0 237 226 217 223 222 219\n",
            "  222 221 216 223 229 215 218 255  77   0]\n",
            " [  0   3   0   0   0   0   0   0   0  62 145 204 228 207 213 221 218 208\n",
            "  211 218 224 223 219 215 224 244 159   0]\n",
            " [  0   0   0   0  18  44  82 107 189 228 220 222 217 226 200 205 211 230\n",
            "  224 234 176 188 250 248 233 238 215   0]\n",
            " [  0  57 187 208 224 221 224 208 204 214 208 209 200 159 245 193 206 223\n",
            "  255 255 221 234 221 211 220 232 246   0]\n",
            " [  3 202 228 224 221 211 211 214 205 205 205 220 240  80 150 255 229 221\n",
            "  188 154 191 210 204 209 222 228 225   0]\n",
            " [ 98 233 198 210 222 229 229 234 249 220 194 215 217 241  65  73 106 117\n",
            "  168 219 221 215 217 223 223 224 229  29]\n",
            " [ 75 204 212 204 193 205 211 225 216 185 197 206 198 213 240 195 227 245\n",
            "  239 223 218 212 209 222 220 221 230  67]\n",
            " [ 48 203 183 194 213 197 185 190 194 192 202 214 219 221 220 236 225 216\n",
            "  199 206 186 181 177 172 181 205 206 115]\n",
            " [  0 122 219 193 179 171 183 196 204 210 213 207 211 210 200 196 194 191\n",
            "  195 191 198 192 176 156 167 177 210  92]\n",
            " [  0   0  74 189 212 191 175 172 175 181 185 188 189 188 193 198 204 209\n",
            "  210 210 211 188 188 194 192 216 170   0]\n",
            " [  2   0   0   0  66 200 222 237 239 242 246 243 244 221 220 193 191 179\n",
            "  182 182 181 176 166 168  99  58   0   0]\n",
            " [  0   0   0   0   0   0   0  40  61  44  72  41  35   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]]\n",
            "\n",
            "학습용 데이터 첫 번째 이미지의 라벨:  9\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OuDHwVBAgWFw",
        "outputId": "fbbb165a-88d9-4c44-d31d-e36a3ff9479c"
      },
      "source": [
        "# 데이터 스케일링\r\n",
        "X_train = X_train / 255.0\r\n",
        "print(X_train[0])\r\n",
        "X_test = X_test / 255.0"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.00392157 0.         0.         0.05098039 0.28627451 0.\n",
            "  0.         0.00392157 0.01568627 0.         0.         0.\n",
            "  0.         0.00392157 0.00392157 0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.01176471 0.         0.14117647 0.53333333 0.49803922 0.24313725\n",
            "  0.21176471 0.         0.         0.         0.00392157 0.01176471\n",
            "  0.01568627 0.         0.         0.01176471]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.02352941 0.         0.4        0.8        0.69019608 0.5254902\n",
            "  0.56470588 0.48235294 0.09019608 0.         0.         0.\n",
            "  0.         0.04705882 0.03921569 0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.60784314 0.9254902  0.81176471 0.69803922\n",
            "  0.41960784 0.61176471 0.63137255 0.42745098 0.25098039 0.09019608\n",
            "  0.30196078 0.50980392 0.28235294 0.05882353]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.00392157\n",
            "  0.         0.27058824 0.81176471 0.8745098  0.85490196 0.84705882\n",
            "  0.84705882 0.63921569 0.49803922 0.4745098  0.47843137 0.57254902\n",
            "  0.55294118 0.34509804 0.6745098  0.25882353]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.00392157 0.00392157 0.00392157\n",
            "  0.         0.78431373 0.90980392 0.90980392 0.91372549 0.89803922\n",
            "  0.8745098  0.8745098  0.84313725 0.83529412 0.64313725 0.49803922\n",
            "  0.48235294 0.76862745 0.89803922 0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.71764706 0.88235294 0.84705882 0.8745098  0.89411765\n",
            "  0.92156863 0.89019608 0.87843137 0.87058824 0.87843137 0.86666667\n",
            "  0.8745098  0.96078431 0.67843137 0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.75686275 0.89411765 0.85490196 0.83529412 0.77647059\n",
            "  0.70588235 0.83137255 0.82352941 0.82745098 0.83529412 0.8745098\n",
            "  0.8627451  0.95294118 0.79215686 0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.00392157 0.01176471 0.\n",
            "  0.04705882 0.85882353 0.8627451  0.83137255 0.85490196 0.75294118\n",
            "  0.6627451  0.89019608 0.81568627 0.85490196 0.87843137 0.83137255\n",
            "  0.88627451 0.77254902 0.81960784 0.20392157]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.02352941 0.\n",
            "  0.38823529 0.95686275 0.87058824 0.8627451  0.85490196 0.79607843\n",
            "  0.77647059 0.86666667 0.84313725 0.83529412 0.87058824 0.8627451\n",
            "  0.96078431 0.46666667 0.65490196 0.21960784]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.01568627 0.         0.\n",
            "  0.21568627 0.9254902  0.89411765 0.90196078 0.89411765 0.94117647\n",
            "  0.90980392 0.83529412 0.85490196 0.8745098  0.91764706 0.85098039\n",
            "  0.85098039 0.81960784 0.36078431 0.        ]\n",
            " [0.         0.         0.00392157 0.01568627 0.02352941 0.02745098\n",
            "  0.00784314 0.         0.         0.         0.         0.\n",
            "  0.92941176 0.88627451 0.85098039 0.8745098  0.87058824 0.85882353\n",
            "  0.87058824 0.86666667 0.84705882 0.8745098  0.89803922 0.84313725\n",
            "  0.85490196 1.         0.30196078 0.        ]\n",
            " [0.         0.01176471 0.         0.         0.         0.\n",
            "  0.         0.         0.         0.24313725 0.56862745 0.8\n",
            "  0.89411765 0.81176471 0.83529412 0.86666667 0.85490196 0.81568627\n",
            "  0.82745098 0.85490196 0.87843137 0.8745098  0.85882353 0.84313725\n",
            "  0.87843137 0.95686275 0.62352941 0.        ]\n",
            " [0.         0.         0.         0.         0.07058824 0.17254902\n",
            "  0.32156863 0.41960784 0.74117647 0.89411765 0.8627451  0.87058824\n",
            "  0.85098039 0.88627451 0.78431373 0.80392157 0.82745098 0.90196078\n",
            "  0.87843137 0.91764706 0.69019608 0.7372549  0.98039216 0.97254902\n",
            "  0.91372549 0.93333333 0.84313725 0.        ]\n",
            " [0.         0.22352941 0.73333333 0.81568627 0.87843137 0.86666667\n",
            "  0.87843137 0.81568627 0.8        0.83921569 0.81568627 0.81960784\n",
            "  0.78431373 0.62352941 0.96078431 0.75686275 0.80784314 0.8745098\n",
            "  1.         1.         0.86666667 0.91764706 0.86666667 0.82745098\n",
            "  0.8627451  0.90980392 0.96470588 0.        ]\n",
            " [0.01176471 0.79215686 0.89411765 0.87843137 0.86666667 0.82745098\n",
            "  0.82745098 0.83921569 0.80392157 0.80392157 0.80392157 0.8627451\n",
            "  0.94117647 0.31372549 0.58823529 1.         0.89803922 0.86666667\n",
            "  0.7372549  0.60392157 0.74901961 0.82352941 0.8        0.81960784\n",
            "  0.87058824 0.89411765 0.88235294 0.        ]\n",
            " [0.38431373 0.91372549 0.77647059 0.82352941 0.87058824 0.89803922\n",
            "  0.89803922 0.91764706 0.97647059 0.8627451  0.76078431 0.84313725\n",
            "  0.85098039 0.94509804 0.25490196 0.28627451 0.41568627 0.45882353\n",
            "  0.65882353 0.85882353 0.86666667 0.84313725 0.85098039 0.8745098\n",
            "  0.8745098  0.87843137 0.89803922 0.11372549]\n",
            " [0.29411765 0.8        0.83137255 0.8        0.75686275 0.80392157\n",
            "  0.82745098 0.88235294 0.84705882 0.7254902  0.77254902 0.80784314\n",
            "  0.77647059 0.83529412 0.94117647 0.76470588 0.89019608 0.96078431\n",
            "  0.9372549  0.8745098  0.85490196 0.83137255 0.81960784 0.87058824\n",
            "  0.8627451  0.86666667 0.90196078 0.2627451 ]\n",
            " [0.18823529 0.79607843 0.71764706 0.76078431 0.83529412 0.77254902\n",
            "  0.7254902  0.74509804 0.76078431 0.75294118 0.79215686 0.83921569\n",
            "  0.85882353 0.86666667 0.8627451  0.9254902  0.88235294 0.84705882\n",
            "  0.78039216 0.80784314 0.72941176 0.70980392 0.69411765 0.6745098\n",
            "  0.70980392 0.80392157 0.80784314 0.45098039]\n",
            " [0.         0.47843137 0.85882353 0.75686275 0.70196078 0.67058824\n",
            "  0.71764706 0.76862745 0.8        0.82352941 0.83529412 0.81176471\n",
            "  0.82745098 0.82352941 0.78431373 0.76862745 0.76078431 0.74901961\n",
            "  0.76470588 0.74901961 0.77647059 0.75294118 0.69019608 0.61176471\n",
            "  0.65490196 0.69411765 0.82352941 0.36078431]\n",
            " [0.         0.         0.29019608 0.74117647 0.83137255 0.74901961\n",
            "  0.68627451 0.6745098  0.68627451 0.70980392 0.7254902  0.7372549\n",
            "  0.74117647 0.7372549  0.75686275 0.77647059 0.8        0.81960784\n",
            "  0.82352941 0.82352941 0.82745098 0.7372549  0.7372549  0.76078431\n",
            "  0.75294118 0.84705882 0.66666667 0.        ]\n",
            " [0.00784314 0.         0.         0.         0.25882353 0.78431373\n",
            "  0.87058824 0.92941176 0.9372549  0.94901961 0.96470588 0.95294118\n",
            "  0.95686275 0.86666667 0.8627451  0.75686275 0.74901961 0.70196078\n",
            "  0.71372549 0.71372549 0.70980392 0.69019608 0.65098039 0.65882353\n",
            "  0.38823529 0.22745098 0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.15686275 0.23921569 0.17254902 0.28235294 0.16078431\n",
            "  0.1372549  0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w65dOWPNhTjf",
        "outputId": "ce532aa0-26c1-4edc-cf8b-ba0b3476147c"
      },
      "source": [
        "# 데이터 모양 정보 전환 (이미지 채널 정보 추가)\r\n",
        "X_train = X_train.reshape(60000, 28, 28, 1) # 1은 채널\r\n",
        "X_test = X_test.reshape(10000, 28, 28, 1)\r\n",
        "\r\n",
        "# 라벨 정보 수정\r\n",
        "Y_train = np_utils.to_categorical(Y_train, 10)\r\n",
        "print('\\n학습용 데이터 첫 번째 이미지의 라벨: ', Y_train[0]) # one-hot 인코딩\r\n",
        "Y_test = np_utils.to_categorical(Y_test, 10)\r\n",
        "\r\n",
        "print('학습용 입력 데이터: ', X_train.shape)\r\n",
        "print('학습용 출력 데이터: ', Y_train.shape)\r\n",
        "\r\n",
        "print('평가용 입력 데이터: ', X_test.shape)\r\n",
        "print('평가용 출력 데이터: ', Y_test.shape)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "학습용 데이터 첫 번째 이미지의 라벨:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            "학습용 입력 데이터:  (60000, 28, 28, 1)\n",
            "학습용 출력 데이터:  (60000, 10)\n",
            "평가용 입력 데이터:  (10000, 28, 28, 1)\n",
            "평가용 출력 데이터:  (10000, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6VW4nNCbkxAg"
      },
      "source": [
        "### 3. 인공 신경망 구현"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4QVtS8sGk0S2",
        "outputId": "cb254e4e-c67a-4ba1-c307-41d420433986"
      },
      "source": [
        "# CNN 구현\r\n",
        "model = Sequential()\r\n",
        "model.add(InputLayer(input_shape=(28,28,1)))\r\n",
        "\r\n",
        "# 첫 번째 합성곱 블럭\r\n",
        "model.add(Conv2D(filters=32, \r\n",
        "                 kernel_size=2, \r\n",
        "                 padding='same', \r\n",
        "                 activation='relu'))\r\n",
        "\r\n",
        "model.add(MaxPool2D(pool_size=2))\r\n",
        "\r\n",
        "# 두 번째 합성곱 블럭\r\n",
        "model.add(Conv2D(filters=64, \r\n",
        "                 kernel_size=2, \r\n",
        "                 padding='same', \r\n",
        "                 activation='relu'))\r\n",
        "\r\n",
        "model.add(MaxPool2D(pool_size=2))\r\n",
        "\r\n",
        "# DNN 입성\r\n",
        "model.add(Flatten())\r\n",
        "\r\n",
        "model.add(Dense(units=128, \r\n",
        "                activation='relu'))\r\n",
        "\r\n",
        "model.add(Dense(units=10,\r\n",
        "                activation='softmax'))\r\n",
        "\r\n",
        "print('CNN 요약')\r\n",
        "model.summary()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CNN 요약\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 28, 28, 32)        160       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 14, 14, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 14, 14, 64)        8256      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 7, 7, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 3136)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 128)               401536    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 411,242\n",
            "Trainable params: 411,242\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSnBBJ-awnE2"
      },
      "source": [
        "### 4. 인공 신경망 학습"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-2J7r-VBwri8",
        "outputId": "d27755d0-5103-41ff-850e-9a649b61eb30"
      },
      "source": [
        "# 학습 방식 결정\r\n",
        "model.compile(optimizer='adam', # adam, sgd\r\n",
        "              loss='categorical_crossentropy', # categorical_crossentropy, mae\r\n",
        "              metrics=['acc'])\r\n",
        "\r\n",
        "# CNN 학습\r\n",
        "print('학습 시작')\r\n",
        "begin = time()\r\n",
        "\r\n",
        "model.fit(x=X_train, \r\n",
        "          y=Y_train, \r\n",
        "          epochs=MY_EPOCH, \r\n",
        "          batch_size=MY_BATCH, \r\n",
        "          verbose=1)\r\n",
        "\r\n",
        "end = time()\r\n",
        "print('학습 시간: {:.2f}'.format(end-begin))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "학습 시작\n",
            "Epoch 1/300\n",
            "600/600 [==============================] - 4s 3ms/step - loss: 0.6752 - acc: 0.7645\n",
            "Epoch 2/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.3325 - acc: 0.8801\n",
            "Epoch 3/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.2792 - acc: 0.8998\n",
            "Epoch 4/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.2440 - acc: 0.9093\n",
            "Epoch 5/300\n",
            "600/600 [==============================] - 1s 2ms/step - loss: 0.2222 - acc: 0.9168\n",
            "Epoch 6/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.2002 - acc: 0.9276\n",
            "Epoch 7/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1803 - acc: 0.9337\n",
            "Epoch 8/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1697 - acc: 0.9387\n",
            "Epoch 9/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1539 - acc: 0.9436\n",
            "Epoch 10/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1341 - acc: 0.9513\n",
            "Epoch 11/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1276 - acc: 0.9535\n",
            "Epoch 12/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1182 - acc: 0.9568\n",
            "Epoch 13/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1021 - acc: 0.9639\n",
            "Epoch 14/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.0906 - acc: 0.9673\n",
            "Epoch 15/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.0780 - acc: 0.9720\n",
            "Epoch 16/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.0733 - acc: 0.9744\n",
            "Epoch 17/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.0598 - acc: 0.9793\n",
            "Epoch 18/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.0504 - acc: 0.9826\n",
            "Epoch 19/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.0475 - acc: 0.9834\n",
            "Epoch 20/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.0386 - acc: 0.9864\n",
            "Epoch 21/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.0328 - acc: 0.9893\n",
            "Epoch 22/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.0304 - acc: 0.9903\n",
            "Epoch 23/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.0301 - acc: 0.9896\n",
            "Epoch 24/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.0243 - acc: 0.9925\n",
            "Epoch 25/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.0241 - acc: 0.9913\n",
            "Epoch 26/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.0193 - acc: 0.9938\n",
            "Epoch 27/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.0144 - acc: 0.9956\n",
            "Epoch 28/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.0218 - acc: 0.9928\n",
            "Epoch 29/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.0179 - acc: 0.9937\n",
            "Epoch 30/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.0160 - acc: 0.9945\n",
            "Epoch 31/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.0112 - acc: 0.9964\n",
            "Epoch 32/300\n",
            "600/600 [==============================] - 1s 2ms/step - loss: 0.0240 - acc: 0.9923\n",
            "Epoch 33/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.0157 - acc: 0.9952\n",
            "Epoch 34/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.0154 - acc: 0.9945\n",
            "Epoch 35/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.0187 - acc: 0.9935\n",
            "Epoch 36/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.0089 - acc: 0.9975\n",
            "Epoch 37/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.0077 - acc: 0.9977\n",
            "Epoch 38/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.0157 - acc: 0.9942\n",
            "Epoch 39/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.0143 - acc: 0.9953\n",
            "Epoch 40/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.0076 - acc: 0.9981\n",
            "Epoch 41/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.0176 - acc: 0.9937\n",
            "Epoch 42/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.0069 - acc: 0.9980\n",
            "Epoch 43/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.0039 - acc: 0.9990\n",
            "Epoch 44/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.0150 - acc: 0.9949\n",
            "Epoch 45/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.0096 - acc: 0.9967\n",
            "Epoch 46/300\n",
            "600/600 [==============================] - 1s 2ms/step - loss: 0.0077 - acc: 0.9976\n",
            "Epoch 47/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.0103 - acc: 0.9965\n",
            "Epoch 48/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.0053 - acc: 0.9984\n",
            "Epoch 49/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.0013 - acc: 0.9998\n",
            "Epoch 50/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.0188 - acc: 0.9936\n",
            "Epoch 51/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.0072 - acc: 0.9973\n",
            "Epoch 52/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.0038 - acc: 0.9988\n",
            "Epoch 53/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.0107 - acc: 0.9967\n",
            "Epoch 54/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.0125 - acc: 0.9955\n",
            "Epoch 55/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.0097 - acc: 0.9966\n",
            "Epoch 56/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.0033 - acc: 0.9990\n",
            "Epoch 57/300\n",
            "600/600 [==============================] - 1s 2ms/step - loss: 0.0145 - acc: 0.9947\n",
            "Epoch 58/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.0050 - acc: 0.9984\n",
            "Epoch 59/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.0034 - acc: 0.9989\n",
            "Epoch 60/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.0119 - acc: 0.9963\n",
            "Epoch 61/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.0085 - acc: 0.9975\n",
            "Epoch 62/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.0079 - acc: 0.9972\n",
            "Epoch 63/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.0074 - acc: 0.9973\n",
            "Epoch 64/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.0103 - acc: 0.9966\n",
            "Epoch 65/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.0039 - acc: 0.9990\n",
            "Epoch 66/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.0020 - acc: 0.9995\n",
            "Epoch 67/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.0150 - acc: 0.9949\n",
            "Epoch 68/300\n",
            "600/600 [==============================] - 1s 2ms/step - loss: 0.0032 - acc: 0.9989\n",
            "Epoch 69/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.0081 - acc: 0.9970\n",
            "Epoch 70/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.0063 - acc: 0.9981\n",
            "Epoch 71/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.0116 - acc: 0.9961\n",
            "Epoch 72/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.0024 - acc: 0.9993\n",
            "Epoch 73/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.0029 - acc: 0.9990\n",
            "Epoch 74/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.0161 - acc: 0.9946\n",
            "Epoch 75/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.0064 - acc: 0.9978\n",
            "Epoch 76/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.0058 - acc: 0.9983\n",
            "Epoch 77/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.0042 - acc: 0.9985\n",
            "Epoch 78/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.0107 - acc: 0.9965\n",
            "Epoch 79/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.0060 - acc: 0.9982\n",
            "Epoch 80/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.0044 - acc: 0.9986\n",
            "Epoch 81/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 6.7724e-04 - acc: 0.9999\n",
            "Epoch 82/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.0130 - acc: 0.9958\n",
            "Epoch 83/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.0047 - acc: 0.9985\n",
            "Epoch 84/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.0070 - acc: 0.9975\n",
            "Epoch 85/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.0062 - acc: 0.9979\n",
            "Epoch 86/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.0080 - acc: 0.9978\n",
            "Epoch 87/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.0041 - acc: 0.9986\n",
            "Epoch 88/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.0013 - acc: 0.9997\n",
            "Epoch 89/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.0089 - acc: 0.9969\n",
            "Epoch 90/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.0081 - acc: 0.9976\n",
            "Epoch 91/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.0045 - acc: 0.9983\n",
            "Epoch 92/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.0044 - acc: 0.9987\n",
            "Epoch 93/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.0051 - acc: 0.9984\n",
            "Epoch 94/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.0045 - acc: 0.9987\n",
            "Epoch 95/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.0094 - acc: 0.9968\n",
            "Epoch 96/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.0027 - acc: 0.9991\n",
            "Epoch 97/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.0044 - acc: 0.9987\n",
            "Epoch 98/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.0027 - acc: 0.9990\n",
            "Epoch 99/300\n",
            "600/600 [==============================] - 1s 2ms/step - loss: 0.0041 - acc: 0.9987\n",
            "Epoch 100/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.0032 - acc: 0.9990\n",
            "Epoch 101/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.0035 - acc: 0.9992\n",
            "Epoch 102/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.0084 - acc: 0.9971\n",
            "Epoch 103/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.0029 - acc: 0.9990\n",
            "Epoch 104/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.0032 - acc: 0.9990\n",
            "Epoch 105/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.0041 - acc: 0.9986\n",
            "Epoch 106/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.0053 - acc: 0.9983\n",
            "Epoch 107/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.0076 - acc: 0.9977\n",
            "Epoch 108/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.0054 - acc: 0.9983\n",
            "Epoch 109/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.0021 - acc: 0.9993\n",
            "Epoch 110/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.0041 - acc: 0.9988\n",
            "Epoch 111/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.0032 - acc: 0.9989\n",
            "Epoch 112/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 8.6005e-04 - acc: 0.9998\n",
            "Epoch 113/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.0104 - acc: 0.9964\n",
            "Epoch 114/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.0015 - acc: 0.9995\n",
            "Epoch 115/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.0066 - acc: 0.9979\n",
            "Epoch 116/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.0054 - acc: 0.9984\n",
            "Epoch 117/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.0013 - acc: 0.9996\n",
            "Epoch 118/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.0026 - acc: 0.9991\n",
            "Epoch 119/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.0077 - acc: 0.9978\n",
            "Epoch 120/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.0047 - acc: 0.9986\n",
            "Epoch 121/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.0048 - acc: 0.9985\n",
            "Epoch 122/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 6.4203e-04 - acc: 0.9998\n",
            "Epoch 123/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.0064 - acc: 0.9979\n",
            "Epoch 124/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.0027 - acc: 0.9991\n",
            "Epoch 125/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.0016 - acc: 0.9994\n",
            "Epoch 126/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 1.2563e-04 - acc: 1.0000\n",
            "Epoch 127/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.0116 - acc: 0.9961\n",
            "Epoch 128/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.0107 - acc: 0.9968\n",
            "Epoch 129/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.0030 - acc: 0.9992\n",
            "Epoch 130/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.0018 - acc: 0.9994\n",
            "Epoch 131/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.0032 - acc: 0.9991\n",
            "Epoch 132/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.0056 - acc: 0.9982\n",
            "Epoch 133/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.0030 - acc: 0.9989\n",
            "Epoch 134/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.0031 - acc: 0.9989\n",
            "Epoch 135/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.0034 - acc: 0.9989\n",
            "Epoch 136/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.0025 - acc: 0.9992\n",
            "Epoch 137/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.0023 - acc: 0.9992\n",
            "Epoch 138/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.0070 - acc: 0.9981\n",
            "Epoch 139/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.0039 - acc: 0.9989\n",
            "Epoch 140/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.0050 - acc: 0.9981\n",
            "Epoch 141/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.0074 - acc: 0.9976\n",
            "Epoch 142/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.0027 - acc: 0.9993\n",
            "Epoch 143/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.0021 - acc: 0.9994\n",
            "Epoch 144/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.0042 - acc: 0.9987\n",
            "Epoch 145/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.0067 - acc: 0.9983\n",
            "Epoch 146/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.0018 - acc: 0.9995\n",
            "Epoch 147/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 3.7928e-04 - acc: 0.9999\n",
            "Epoch 148/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.0071 - acc: 0.9974\n",
            "Epoch 149/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.0075 - acc: 0.9978\n",
            "Epoch 150/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.0014 - acc: 0.9997\n",
            "Epoch 151/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.0046 - acc: 0.9985\n",
            "Epoch 152/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.0026 - acc: 0.9992\n",
            "Epoch 153/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.0044 - acc: 0.9988\n",
            "Epoch 154/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.0029 - acc: 0.9991\n",
            "Epoch 155/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.0023 - acc: 0.9992\n",
            "Epoch 156/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.0020 - acc: 0.9995\n",
            "Epoch 157/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.0099 - acc: 0.9975\n",
            "Epoch 158/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.0035 - acc: 0.9989\n",
            "Epoch 159/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.0077 - acc: 0.9976\n",
            "Epoch 160/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.0011 - acc: 0.9998\n",
            "Epoch 161/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 3.2055e-04 - acc: 0.9998\n",
            "Epoch 162/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.0115 - acc: 0.9968\n",
            "Epoch 163/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.0034 - acc: 0.9991\n",
            "Epoch 164/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.0023 - acc: 0.9995\n",
            "Epoch 165/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 9.7305e-04 - acc: 0.9997\n",
            "Epoch 166/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.0038 - acc: 0.9990\n",
            "Epoch 167/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.0042 - acc: 0.9987\n",
            "Epoch 168/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.0060 - acc: 0.9981\n",
            "Epoch 169/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.0039 - acc: 0.9989\n",
            "Epoch 170/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 9.1047e-04 - acc: 0.9996\n",
            "Epoch 171/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.0040 - acc: 0.9986\n",
            "Epoch 172/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.0060 - acc: 0.9982\n",
            "Epoch 173/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.0026 - acc: 0.9991\n",
            "Epoch 174/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.0040 - acc: 0.9988\n",
            "Epoch 175/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.0032 - acc: 0.9990\n",
            "Epoch 176/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.0030 - acc: 0.9992\n",
            "Epoch 177/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.0017 - acc: 0.9996\n",
            "Epoch 178/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.0035 - acc: 0.9988\n",
            "Epoch 179/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.0032 - acc: 0.9986\n",
            "Epoch 180/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 1.8464e-04 - acc: 1.0000\n",
            "Epoch 181/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 1.2997e-05 - acc: 1.0000\n",
            "Epoch 182/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 5.3413e-06 - acc: 1.0000\n",
            "Epoch 183/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 4.3322e-06 - acc: 1.0000\n",
            "Epoch 184/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 3.4283e-06 - acc: 1.0000\n",
            "Epoch 185/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 2.8688e-06 - acc: 1.0000\n",
            "Epoch 186/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 2.6617e-06 - acc: 1.0000\n",
            "Epoch 187/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 1.8837e-06 - acc: 1.0000\n",
            "Epoch 188/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 1.5507e-06 - acc: 1.0000\n",
            "Epoch 189/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 1.1887e-06 - acc: 1.0000\n",
            "Epoch 190/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 1.0302e-06 - acc: 1.0000\n",
            "Epoch 191/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 8.4451e-07 - acc: 1.0000\n",
            "Epoch 192/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 7.3371e-07 - acc: 1.0000\n",
            "Epoch 193/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 5.2822e-07 - acc: 1.0000\n",
            "Epoch 194/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 5.0756e-07 - acc: 1.0000\n",
            "Epoch 195/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 3.9783e-07 - acc: 1.0000\n",
            "Epoch 196/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 3.0365e-07 - acc: 1.0000\n",
            "Epoch 197/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 2.3609e-07 - acc: 1.0000\n",
            "Epoch 198/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 2.0680e-07 - acc: 1.0000\n",
            "Epoch 199/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 1.6306e-07 - acc: 1.0000\n",
            "Epoch 200/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 1.1839e-07 - acc: 1.0000\n",
            "Epoch 201/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 1.0115e-07 - acc: 1.0000\n",
            "Epoch 202/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 7.4682e-08 - acc: 1.0000\n",
            "Epoch 203/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 6.6819e-08 - acc: 1.0000\n",
            "Epoch 204/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 5.1612e-08 - acc: 1.0000\n",
            "Epoch 205/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 3.7185e-08 - acc: 1.0000\n",
            "Epoch 206/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 3.0720e-08 - acc: 1.0000\n",
            "Epoch 207/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 2.3882e-08 - acc: 1.0000\n",
            "Epoch 208/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 1.9037e-08 - acc: 1.0000\n",
            "Epoch 209/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 1.5204e-08 - acc: 1.0000\n",
            "Epoch 210/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 1.2647e-08 - acc: 1.0000\n",
            "Epoch 211/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 1.0120e-08 - acc: 1.0000\n",
            "Epoch 212/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 8.6370e-09 - acc: 1.0000\n",
            "Epoch 213/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 7.3657e-09 - acc: 1.0000\n",
            "Epoch 214/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 5.8589e-09 - acc: 1.0000\n",
            "Epoch 215/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 4.7233e-09 - acc: 1.0000\n",
            "Epoch 216/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 3.8837e-09 - acc: 1.0000\n",
            "Epoch 217/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 3.4346e-09 - acc: 1.0000\n",
            "Epoch 218/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 2.7983e-09 - acc: 1.0000\n",
            "Epoch 219/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 2.5139e-09 - acc: 1.0000\n",
            "Epoch 220/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 2.1798e-09 - acc: 1.0000\n",
            "Epoch 221/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 1.7957e-09 - acc: 1.0000\n",
            "Epoch 222/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 1.4406e-09 - acc: 1.0000\n",
            "Epoch 223/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 1.3488e-09 - acc: 1.0000\n",
            "Epoch 224/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 1.2729e-09 - acc: 1.0000\n",
            "Epoch 225/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 1.0437e-09 - acc: 1.0000\n",
            "Epoch 226/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 9.2049e-10 - acc: 1.0000\n",
            "Epoch 227/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 8.5456e-10 - acc: 1.0000\n",
            "Epoch 228/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 8.0538e-10 - acc: 1.0000\n",
            "Epoch 229/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 6.5044e-10 - acc: 1.0000\n",
            "Epoch 230/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 5.8214e-10 - acc: 1.0000\n",
            "Epoch 231/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 5.1835e-10 - acc: 1.0000\n",
            "Epoch 232/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 5.4491e-10 - acc: 1.0000\n",
            "Epoch 233/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 4.5073e-10 - acc: 1.0000\n",
            "Epoch 234/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 4.3101e-10 - acc: 1.0000\n",
            "Epoch 235/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 4.0867e-10 - acc: 1.0000\n",
            "Epoch 236/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 4.1705e-10 - acc: 1.0000\n",
            "Epoch 237/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 4.1908e-10 - acc: 1.0000\n",
            "Epoch 238/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 3.3240e-10 - acc: 1.0000\n",
            "Epoch 239/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 3.5025e-10 - acc: 1.0000\n",
            "Epoch 240/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 3.1815e-10 - acc: 1.0000\n",
            "Epoch 241/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 3.0829e-10 - acc: 1.0000\n",
            "Epoch 242/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 2.9003e-10 - acc: 1.0000\n",
            "Epoch 243/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 3.0547e-10 - acc: 1.0000\n",
            "Epoch 244/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 3.0286e-10 - acc: 1.0000\n",
            "Epoch 245/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 3.0191e-10 - acc: 1.0000\n",
            "Epoch 246/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 2.9438e-10 - acc: 1.0000\n",
            "Epoch 247/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 2.3277e-10 - acc: 1.0000\n",
            "Epoch 248/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 1.9019e-10 - acc: 1.0000\n",
            "Epoch 249/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 2.3784e-10 - acc: 1.0000\n",
            "Epoch 250/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 2.3558e-10 - acc: 1.0000\n",
            "Epoch 251/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 2.2285e-10 - acc: 1.0000\n",
            "Epoch 252/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 2.5272e-10 - acc: 1.0000\n",
            "Epoch 253/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 2.0120e-10 - acc: 1.0000\n",
            "Epoch 254/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 1.9070e-10 - acc: 1.0000\n",
            "Epoch 255/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 1.7775e-10 - acc: 1.0000\n",
            "Epoch 256/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 1.3958e-10 - acc: 1.0000\n",
            "Epoch 257/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 1.7353e-10 - acc: 1.0000\n",
            "Epoch 258/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 1.5699e-10 - acc: 1.0000\n",
            "Epoch 259/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 1.7416e-10 - acc: 1.0000\n",
            "Epoch 260/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 1.7180e-10 - acc: 1.0000\n",
            "Epoch 261/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 1.4822e-10 - acc: 1.0000\n",
            "Epoch 262/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 1.5094e-10 - acc: 1.0000\n",
            "Epoch 263/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 1.3498e-10 - acc: 1.0000\n",
            "Epoch 264/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 1.4351e-10 - acc: 1.0000\n",
            "Epoch 265/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 1.5950e-10 - acc: 1.0000\n",
            "Epoch 266/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 1.1941e-10 - acc: 1.0000\n",
            "Epoch 267/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 1.7300e-10 - acc: 1.0000\n",
            "Epoch 268/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 1.3672e-10 - acc: 1.0000\n",
            "Epoch 269/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 1.0730e-10 - acc: 1.0000\n",
            "Epoch 270/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 1.1379e-10 - acc: 1.0000\n",
            "Epoch 271/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 1.4810e-10 - acc: 1.0000\n",
            "Epoch 272/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 1.2068e-10 - acc: 1.0000\n",
            "Epoch 273/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 1.5121e-10 - acc: 1.0000\n",
            "Epoch 274/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 1.1430e-10 - acc: 1.0000\n",
            "Epoch 275/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 9.9720e-11 - acc: 1.0000\n",
            "Epoch 276/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 1.2292e-10 - acc: 1.0000\n",
            "Epoch 277/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 1.4409e-10 - acc: 1.0000\n",
            "Epoch 278/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 1.0770e-10 - acc: 1.0000\n",
            "Epoch 279/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 1.1147e-10 - acc: 1.0000\n",
            "Epoch 280/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 8.8835e-11 - acc: 1.0000\n",
            "Epoch 281/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 9.0322e-11 - acc: 1.0000\n",
            "Epoch 282/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 1.4695e-10 - acc: 1.0000\n",
            "Epoch 283/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 1.1178e-10 - acc: 1.0000\n",
            "Epoch 284/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 9.0565e-11 - acc: 1.0000\n",
            "Epoch 285/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 1.1248e-10 - acc: 1.0000\n",
            "Epoch 286/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 9.0333e-11 - acc: 1.0000\n",
            "Epoch 287/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 9.8787e-11 - acc: 1.0000\n",
            "Epoch 288/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 1.1714e-10 - acc: 1.0000\n",
            "Epoch 289/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 1.0665e-10 - acc: 1.0000\n",
            "Epoch 290/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 9.4606e-11 - acc: 1.0000\n",
            "Epoch 291/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 8.7912e-11 - acc: 1.0000\n",
            "Epoch 292/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 9.4004e-11 - acc: 1.0000\n",
            "Epoch 293/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 9.7317e-11 - acc: 1.0000\n",
            "Epoch 294/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 8.0214e-11 - acc: 1.0000\n",
            "Epoch 295/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 7.2421e-11 - acc: 1.0000\n",
            "Epoch 296/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 6.9962e-11 - acc: 1.0000\n",
            "Epoch 297/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 8.6146e-11 - acc: 1.0000\n",
            "Epoch 298/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 1.0437e-10 - acc: 1.0000\n",
            "Epoch 299/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 9.4931e-11 - acc: 1.0000\n",
            "Epoch 300/300\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 8.5676e-11 - acc: 1.0000\n",
            "학습 시간: 470.48\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BN5S7GedyFjs"
      },
      "source": [
        "### 5. CNN 평가"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ftBEAdUeyJHl",
        "outputId": "959891ce-5fdc-4f24-8cac-a4cff58c1325"
      },
      "source": [
        "# CNN 평가\r\n",
        "score = model.evaluate(x=X_test, \r\n",
        "                       y=Y_test, \r\n",
        "                       verbose=1)\r\n",
        "\r\n",
        "print('최종 손실 값: {:.2f}'.format(score[0]))\r\n",
        "print('최종 정확도: {:.2f}'.format(score[1]))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 1s 3ms/step - loss: 2.0099 - acc: 0.9198\n",
            "최종 손실 값: 2.01\n",
            "최종 정확도: 0.92\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBa-lsmBy7qu"
      },
      "source": [
        "### 6. CNN 예측"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jS2p4h2gzC3V",
        "outputId": "46450a10-caf9-4a9e-aeac-ee68c8b6fb39"
      },
      "source": [
        "# 샘플 이미지 지정\r\n",
        "image = X_test[1234]\r\n",
        "print(image.shape)\r\n",
        "\r\n",
        "# batch 정보 추가\r\n",
        "image = image.reshape(1, 28, 28, 1)\r\n",
        "pred = model.predict(image)\r\n",
        "\r\n",
        "# 정보 출력\r\n",
        "print('CNN 예측 값: ', pred)\r\n",
        "print('정답: ', Y_test[1234])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(28, 28, 1)\n",
            "CNN 예측 값:  [[8.0323523e-28 0.0000000e+00 9.9994218e-01 0.0000000e+00 5.7799618e-05\n",
            "  0.0000000e+00 1.6886728e-14 0.0000000e+00 0.0000000e+00 1.4256447e-32]]\n",
            "정답:  [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 610
        },
        "id": "1Yf1Pw9_4vhl",
        "outputId": "0af4a053-0159-4c7b-8187-443931c9a05e"
      },
      "source": [
        "plt.figure(figsize=(10,10))\r\n",
        "tmp = X_train[1234].reshape(28,28)\r\n",
        "tmp = tmp * 255\r\n",
        "plt.imshow(tmp, cmap='gray')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f3c17f9f518>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rajlbsTxVOs1"
      },
      "source": [
        "### GPU 확인"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EgUBJob7VMbt",
        "outputId": "decb7e20-324c-4533-927e-e6c059c96933"
      },
      "source": [
        "gpu_info = !nvidia-smi\r\n",
        "gpu_info = '\\n'.join(gpu_info)\r\n",
        "if gpu_info.find('failed') >= 0:\r\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\r\n",
        "  print('and then re-execute this cell.')\r\n",
        "else:\r\n",
        "  print(gpu_info)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sun Jan 10 15:17:46 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.27.04    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   44C    P0    42W / 300W |   1127MiB / 16130MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}